---
title: "Pipelines with several MCMC runs"
output: rmarkdown::html_vignette
bibliography: mcmc_rep.bib
vignette: >
  %\VignetteIndexEntry{Pipelines with several MCMC runs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_knit$set(root.dir = fs::dir_create(tempfile()))
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
if (identical(Sys.getenv("NOT_CRAN", unset = "false"), "false")) {
  knitr::opts_chunk$set(eval = FALSE)
}
library(R2jags)
library(dplyr)
library(targets)
library(jagstargets)
```

It is sometimes desirable to run one or more Bayesian models repeatedly across multiple simulated datasets. Examples:

1. Validate the implementation of a Bayesian model using simulation-based calibration [SBC; @cook2006; @talts2020].
2. Simulate a randomized controlled experiment to explore frequentist properties such as power and Type I error.

This vignette focuses on (1). The goal is to simulate multiple datasets from the model below, analyze each dataset, and assess how often the estimated credible intervals for `beta` capture the true value of `beta` from the simulation.

```{r}
lines <- "model {
  for (i in 1:n) {
    y[i] ~ dnorm(x[i] * beta, 1)
  }
  beta ~ dnorm(0, 1)
}"
writeLines(lines, "model.jags")
```

Next, we define a pipeline to simulate multiple datasets and fit each dataset with the model. Below, we commit to 10 replications: 2 batches with 5 iterations per batch. (In practical situations, the total number of replications should be hundreds of times more.) We also supply custom variable names and summary functions to return the 95% credible intervals for `beta`. Below, we use the `data_copy` argument to copy the true value of `beta` from the data to the results, which will help us assess the credible intervals.

```{r, echo = FALSE}
library(targets)
tar_script({
  library(jagstargets)
  options(crayon.enabled = FALSE)
  tar_option_set(memory = "transient", garbage_collection = TRUE)
  list(
    tar_jags_rep_summary(
      model,
      "model.jags",
      data = tar_jags_example_data(),
      parameters.to.save = "beta",
      batches = 5, # Number of branch targets.
      reps = 2, # Number of model reps per branch target.
      log = R.utils::nullfile(),
      data_copy = "true_beta", # Append scalars from data to the output data frame.
      data_omit = "true_beta", # Omit true_beta from the MCMC (avoids warnings).
      variables = "beta",
      summaries = list(
        ~posterior::quantile2(.x, probs = c(0.025, 0.975))
      )
    )
  )
})
```

```{r, eval = FALSE}
# _targets.R
library(targets)
library(jagstargets)
options(crayon.enabled = FALSE)
tar_option_set(memory = "transient", garbage_collection = TRUE)

generate_data <- function() {
  true_beta <- stats::rnorm(n = 1, mean = 0, sd = 1)
  x <- seq(from = -1, to = 1, length.out = n)
  y <- stats::rnorm(n, x * true_beta, 1)
  out <- list(n = n, x = x, y = y, true_beta = true_beta)
}

list(
  tar_jags_rep_summary(
    model,
    "model.jags",
    data = generate_data(),
    parameters.to.save = "beta",
    batches = 5, # Number of branch targets.
    reps = 2, # Number of model reps per branch target.
    log = R.utils::nullfile(),
    data_copy = "true_beta", # Append scalars from data to the output data frame.
    data_omit = "true_beta", # Omit true_beta from the MCMC (avoids warnings).
    variables = "beta",
    summaries = list(
      ~posterior::quantile2(.x, probs = c(0.025, 0.975))
    )
  )
)
```

We now have a pipeline that runs the model 10 times: 5 batches (branch targets) with 2 replications per batch.

```{r}
tar_visnetwork(targets_only = TRUE)
```

Run the computation with `tar_make()`

```{r, output = FALSE, warning = FALSE}
tar_make()
```

The result is an aggregated data frame of summary statistics, where the `.rep` column distinguishes among individual replicates. We have the credible intervals for `beta` in columns `q2.5` and `q97.5`. And thanks to `data_copy`, we also have `true_beta`, the value of `beta` used to generate the dataset in each simulation rep.

```{r}
tar_load(model)
model
```

Now, let's assess how often the estimated 95% credible intervals capture the true values of `beta`. If the model is implemented correctly, the coverage value below should be close to 95%. (Ordinarily, we would [increase the number of batches and reps per batch](https://wlandau.github.io/targets-manual/dynamic.html#batching) and [run batches in parallel computing](https://wlandau.github.io/targets-manual/hpc.html).)

```{r}
library(dplyr)
model %>%
  summarize(coverage = mean(q2.5 < true_beta & true_beta < q97.5))
```
For maximum reproducibility, we should express the coverage assessment as a custom function and a target in the pipeline.

```{r, echo = FALSE}
library(targets)
tar_script({
  library(jagstargets)
  options(crayon.enabled = FALSE)
  tar_option_set(
    packages = "dplyr",
    memory = "transient",
    garbage_collection = TRUE
  )
  list(
    tar_jags_rep_summary(
      model,
      "model.jags",
      data = tar_jags_example_data(),
      parameters.to.save = "beta",
      batches = 5, # Number of branch targets.
      reps = 2, # Number of model reps per branch target.
      log = R.utils::nullfile(),
      data_copy = "true_beta", # Append scalars from data to the output data frame.
      data_omit = "true_beta", # Omit true_beta from the MCMC (avoids warnings).
      variables = "beta",
      summaries = list(
        ~posterior::quantile2(.x, probs = c(0.025, 0.975))
      )
    ),
    tar_target(
      coverage,
      model %>%
        summarize(coverage = mean(q2.5 < true_beta & true_beta < q97.5))
    )
  )
})
```

```{r, eval = FALSE}
# _targets.R
library(targets)
library(jagstargets)

generate_data <- function() {
  true_beta <- stats::rnorm(n = 1, mean = 0, sd = 1)
  x <- seq(from = -1, to = 1, length.out = n)
  y <- stats::rnorm(n, x * true_beta, 1)
  out <- list(n = n, x = x, y = y, true_beta = true_beta)
}

list(
  tar_jags_rep_summary(
    model,
    "model.jags",
    data = generate_data(),
    parameters.to.save = "beta",
    batches = 5, # Number of branch targets.
    reps = 2, # Number of model reps per branch target.
    log = R.utils::nullfile(),
    data_copy = "true_beta", # Append scalars from data to the output data frame.
    data_omit = "true_beta", # Omit true_beta from the MCMC (avoids warnings).
    variables = "beta",
    summaries = list(
      ~posterior::quantile2(.x, probs = c(0.025, 0.975))
    )
  ),
  tar_target(
    coverage,
    model %>%
      summarize(coverage = mean(q2.5 < true_beta & true_beta < q97.5))
  )
)
```

The new `coverage` target should the only outdated target, and it should be connected to the upstream `model` target.

```{r}
tar_visnetwork(targets_only = TRUE)
```

When we run the pipeline, only the coverage assessment should run. That way, we skip all the expensive computation of simulating datasets and running MCMC multiple times.

```{r, output = FALSE, warning = FALSE}
tar_make()
```

```{r}
tar_read(coverage)
```

## Multiple models

`tar_jags_rep_mcmc_summary()` and similar functions allow you to supply multiple jags models. If you do, each model will share the the same collection of datasets. Below, we add a new `model2.jags` file to the `jags_files` argument of `tar_jags_rep_mcmc_summary()`. In the coverage summary below, we group by `.name` to compute a coverage statistic for each model.


```{r, echo = FALSE}
library(targets)
tar_script({
  library(jagstargets)
  options(crayon.enabled = FALSE)
  tar_option_set(
    packages = "dplyr",
    memory = "transient",
    garbage_collection = TRUE
  )
  list(
    tar_jags_rep_summary(
      model,
      c("model.jags", "model2.jags"), # another model
      data = tar_jags_example_data(),
      parameters.to.save = "beta",
      batches = 5,
      reps = 2,
      log = R.utils::nullfile(),
      data_copy = "true_beta",
      data_omit = "true_beta",
      variables = "beta",
      summaries = list(
        ~posterior::quantile2(.x, probs = c(0.025, 0.975))
      )
    ),
    tar_target(
      coverage,
      model %>%
        group_by(.name) %>%
        summarize(coverage = mean(q2.5 < true_beta & true_beta < q97.5))
    )
  )
})
```

```{r, eval = FALSE}
# _targets.R
library(targets)
library(jagstargets)

generate_data <- function() {
  true_beta <- stats::rnorm(n = 1, mean = 0, sd = 1)
  x <- seq(from = -1, to = 1, length.out = n)
  y <- stats::rnorm(n, x * true_beta, 1)
  out <- list(n = n, x = x, y = y, true_beta = true_beta)
}

list(
  tar_jags_rep_summary(
    model,
    c("model.jags", "model2.jags"), # another model
    data = generate_data(),
    parameters.to.save = "beta",
    batches = 5,
    reps = 2,
    log = R.utils::nullfile(),
    data_copy = "true_beta",
    data_omit = "true_beta",
    variables = "beta",
    summaries = list(
      ~posterior::quantile2(.x, probs = c(0.025, 0.975))
    )
  ),
  tar_target(
    coverage,
    model %>%
      group_by(.name) %>%
      summarize(coverage = mean(q2.5 < true_beta & true_beta < q97.5))
  )
)
```

In the graph below, notice how targets `model_model1` and `model_model2` are both connected to `model_data` upstream. Downstream, `model` is equivalent to `dplyr::bind_rows(model_model1, model_model2)`, and it will have special columns `.name` and `.file` to distinguish among all the models.

```{r}
tar_visnetwork(targets_only = TRUE)
```

## References
